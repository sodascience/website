











































































[{"categories":null,"contents":"Through ODISSEI, researchers can get access to microdata of the Dutch national statistical agency Statistics Netherlands (CBS). This access is highly restricted, ensuring privacy of the inhabitants of the Netherlands. However, this restriction is at odds with reproducibility, a key open science research practice: analyses done using these microdatasets cannot be easily reproduced by others. In this post, we describe a workflow to enhance the reproducibility of CBS microdata analyses done on the Remote Access (RA) environment by creating and publishing synthetic data. For this, we will be using our new synthpop.extract R package.\nThe general procedure of the workflow is as follows:\nPreparation\nInstall synthpop.extract on the remote access (RA) environment and on your own computer. On the CBS RA environment:\nEstimate a parametric generative model for the dataset using the well-known synthpop package in R. Extract parameters of this generative model and ensure disclosure control according to the CBS output guidelines using synthpop.extract. Export the disclosure controlled parameters to an xlsx file using synthpop.extract. Offer the xlsx file to the microdata team for output control. Then, outside the CBS RA environment:\nDownload the xlsx file and load the parameters back in R using synthpop.extract. Generate synthetic data based on the model parameters using synthpop.extract. Run the original analysis on the synthetic data and report differences. Below, we perform this procedure using an example: confirmatory factor analysis on a personality trait questionnaire dataset.\nInstallation Install the synthpop.extract package on your own computer using the following code.\n# Install the package remotes::install.packages(\u0026#34;sodascience/synthpop.extract\u0026#34;) # Load the package (this also loads the synthpop package!) library(synthpop.extract) On the RA environment, you first need to upload the package as a folder (subject to checks by the RA team). This folder can be downloaded from the releases page here. Once the folder is on the RA environment, unzip it to a nice location and install the package as follows:\ninstall.packages(\u0026#34;C:/path/to/synthpop.extract\u0026#34;, repos = NULL, type = \u0026#34;source\u0026#34;) Estimate parametric generative model As the first step to generate synthetic data, we use the synthpop package. Specifically, we will estimate a parametric conditional regression model for the dataset of interest. If you want to know more about synthpop and how to generate data, see synthpop.org.uk.\nFor this example, we use the data big5, which is included in the synthpop.extract package. Load the data and call the syn function as follows:\nsynds \u0026lt;- syn(big5, method = \u0026#34;norm\u0026#34;, models = TRUE) Make sure to set models = TRUE when calling the syn function, as that ensures that the model parameters are stored in the output object (synds). Our big5 data is continuous and roughly multivariate normal, hence we chose method = \u0026quot;norm\u0026quot;. For different data types, we support the following methods:\nmethod variable type \u0026quot;norm\u0026quot; continuous variables \u0026quot;logreg\u0026quot; binary variables \u0026quot;polyreg\u0026quot; unordered categorical variables \u0026quot;polr\u0026quot; ordered categorical variables Note: the variable names of your dataset should not include numbers due to a bug in the synthpop package when models = TRUE.\nExtract parameters After creating our model, we use the synp_get_param function, which takes the original data and the synds object as arguments to create a list of the parameters of the generative models:\nmodel_par \u0026lt;- synp_get_param(big5, synds) The synp_get_param function performs some checks to see if the process meets the disclosure control conditions according to the CBS output guidelines. For example, if you create these parameters based on only 15 observations, you will get the following error:\nError in synp_get_param(big5[1:15, ], synds) : Disclosure control (as per CBS guideline #2): EST_I should have minimum 10 degrees of freedom to proceed. Note: although synthpop.extract mostly ensures that output guidelines are met, we cannot guarantee that all edge cases are included. The data analyst maintains the full responsibility of ensuring privacy in the output stage.\nSave to Excel Next, we export the disclosure controlled parameters to an xlsx file using the synp_write_sheets function.\nsynp_write_sheets(model_par, \u0026#34;big5.xlsx\u0026#34;) You can check to see that all the parameters are indeed written to the excel sheet. If you want to perform additional disclosure control steps, you can do this directly in the excel file (but be sure to not change its structure!).\nExport from CBS RA environment The next step is to offer the xlsx file to the microdata team for output control. For this, store the big5.xlsx file in the output folder of the CBS RA environment. After this file passes the check, it can be downloaded and stored on your own computer.\nLoad the parameters in the Excel file into R With our outputted big5.xlsx file downloaded to our own computer, we can now load the parameters back in R using synp_read_sheets function.\nmodel_par \u0026lt;- synp_read_sheets(\u0026#34;path/to/big5.xlsx\u0026#34;) Generate synthetic data Now, we can finally generate the synthetic data using synp_gen_syndat function. It takes the list of parameters resulting from synp_read_sheets() and the sample size n as arguments (if not specified, default n = 1000 is applied). Our example big5 dataset has 100000 rows, so we generate the same amount in this step.\nsyndat \u0026lt;- synp_gen_syndat(model_par, n = 100000) Check the synthetic data. # show the first 6 rows and columns head(syndat[, 1:6]) EXT_I EXT_II EXT_III EXT_IV EXT_V EST_I 1 1.7967667 4.5696000 2.470643 3.594090 4.198519 2.772357 2 1.5007013 3.3665465 3.995861 3.043404 1.888603 2.725456 3 0.2680578 3.3021180 4.042961 2.081508 2.697252 1.983089 4 2.1136429 2.6683560 2.328611 3.324001 2.376048 0.606386 5 1.9526136 0.2552039 2.827340 1.287563 2.680024 3.749983 6 4.3846772 4.3583375 2.510808 3.943423 2.911146 2.809992 Compare analyses on synthetic \u0026amp; real data Before publishing the synthetic data, it is important to show how well the analysis you\u0026rsquo;re interested in replicates in the synthetic data. We recommend briefly summarizing the differences in the main analysis for potential users of your code. For our big5 example, we perform confirmatory factor analysis on both the synthesized and original data. Then, we compare the results to see if they lead to more or less the same conclusion.\nAs seen below in the table, it is observed that the resulting estimates from the factor analysis on the synthetic data coincide closely with those from the analysis on the original data.\nFactor loading Original data (se) Synthetic data (se) EXTRA =~ EXT_IV 0.858 (.004) 0.860 (.004) EXTRA =~ EXT_V 0.959 (.004) 0.953 (.004) AGREE =~ AGR_IV 0.788 (.004) 0.787 (.004) AGREE =~ AGR_V 0.796 (.004) 0.797 (.004) EMO =~ EST_IV 0.438 (.004) 0.439 (.004) EMO =~ EST_V 0.557 (.004) 0.561 (.004) OPEN =~ OPN_IV 0.708 (.004) 0.702 (.004) OPEN =~ OPN_V 0.440 (.004) 0.441 (.004) CON =~ CSN_IV 0.808 (.004) 0.808 (.004) CON =~ CSN_V 0.789 (.004) 0.782 (.004) Note: these analyses align closely because Confirmatory Factor Analysis is linear and assumes normality, and the model we used to generate the synthetic data does too. With more complicated analyses, you will likely see larger deviations from the original analysis!\nConclusion In this tutorial, we have shown how to estimate a parametric model for your data, extract and store its parameters, and then produce synthetic data based on this model. Publishing synthetic data along with your analysis script will improve the reproducibility and understandability of your analyses, so we recommend doing so!\nAppendix: Detailed description of performing factor analysis library(tidyverse) library(skimr) library(lavaan) library(lavaanPlot) Example dataset As an example analysis, we perform factor analysis for the big five personality traits. For the detailed description of the data, please check the help file (e.g., ?big5). We can take a quick look at the data summary as below.\n## data summary skim(big5) See the data summary. Name big5 Number of rows 100000 Number of columns 25 _______________________ Column type frequency: numeric 25 ________________________ Group variables None Variable type: numeric\nvariable n_missing mean sd p25 p50 p75 hist EXT1 0 2.64 1.26 1 3 4 ▇▆▇▅▂ EXT2 0 3.21 1.31 2 3 4 ▅▆▇▇▇ EXT3 0 3.29 1.22 2 3 4 ▂▆▇▇▅ EXT4 0 2.84 1.21 2 3 4 ▅▇▇▆▃ EXT5 0 3.28 1.28 2 3 4 ▃▅▆▇▅ EST1 0 2.69 1.32 2 3 4 ▇▇▆▆▃ EST2 0 3.17 1.23 2 3 4 ▃▆▇▇▅ EST3 0 2.14 1.13 1 2 3 ▇▇▃▂▁ EST4 0 2.66 1.25 2 3 4 ▆▇▇▅▂ EST5 0 3.14 1.25 2 3 4 ▃▇▇▇▅ AGR1 0 3.73 1.33 3 4 5 ▂▃▃▅▇ AGR2 0 3.83 1.14 3 4 5 ▁▂▅▇▇ AGR3 0 3.73 1.27 3 4 5 ▁▃▃▅▇ AGR4 0 3.93 1.13 3 4 5 ▁▂▃▇▇ AGR5 0 3.71 1.16 3 4 5 ▁▂▅▇▆ CSN1 0 3.29 1.18 3 3 4 ▂▅▆▇▃ CSN2 0 3.05 1.37 2 3 4 ▅▇▆▇▆ CSN3 0 3.98 1.04 3 4 5 ▁▂▃▇▇ CSN4 0 3.36 1.23 2 3 4 ▂▅▆▇▆ CSN5 0 2.62 1.27 2 2 4 ▇▇▇▅▃ OPN1 0 3.65 1.16 3 4 5 ▁▂▆▇▇ OPN2 0 3.91 1.10 3 4 5 ▁▂▅▇▇ OPN3 0 4.00 1.10 3 4 5 ▁▂▃▆▇ OPN4 0 3.98 1.07 3 4 5 ▁▁▃▆▇ OPN5 0 3.79 0.99 3 4 5 ▁▁▆▇▅ Perform factor analysis We first run factor analysis on the original data.\n## lavaan model syntax syntax \u0026lt;- \u0026#34; EXTRA =~ EXT_I + EXT_II + EXT_III + EXT_IV + EXT_V AGREE =~ AGR_I + AGR_II + AGR_III + AGR_IV + AGR_V EMO =~ EST_I + EST_II + EST_III + EST_IV + EST_V OPEN =~ OPN_I + OPN_II + OPN_III + OPN_IV + OPN_V CON =~ CSN_I + CSN_II + CSN_III + CSN_IV + CSN_V \u0026#34; ## run CFA on original data original_CFA \u0026lt;- cfa(model = syntax, data = big5, std.lv = TRUE) ## assess the fit indices fitMeasures(original_CFA, fit.measures = c(\u0026#34;cfi\u0026#34;,\u0026#34;srmr\u0026#34;, \u0026#34;rmsea\u0026#34;)) cfi srmr rmsea 0.813 0.066 0.066 Check the CFA results in detail. lavaan 0.6-12 ended normally after 19 iterations Estimator ML Optimization method NLMINB Number of model parameters 60 Number of observations 100000 Model Test User Model: Test statistic 116396.465 Degrees of freedom 265 P-value (Chi-square) 0.000 Model Test Baseline Model: Test statistic 621980.047 Degrees of freedom 300 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.813 Tucker-Lewis Index (TLI) 0.789 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3741032.528 Loglikelihood unrestricted model (H1) -3682834.295 Akaike (AIC) 7482185.055 Bayesian (BIC) 7482755.831 Sample-size adjusted Bayesian (BIC) 7482565.149 Root Mean Square Error of Approximation: RMSEA 0.066 90 Percent confidence interval - lower 0.066 90 Percent confidence interval - upper 0.067 P-value RMSEA \u0026lt;= 0.05 0.000 Standardized Root Mean Square Residual: SRMR 0.066 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all EXTRA =~ EXT_I 0.838 0.004 217.930 0.000 0.838 0.662 EXT_II 0.903 0.004 230.160 0.000 0.903 0.691 EXT_III 0.853 0.004 234.583 0.000 0.853 0.701 EXT_IV 0.860 0.004 239.410 0.000 0.860 0.712 EXT_V 0.953 0.004 254.331 0.000 0.953 0.746 AGREE =~ AGR_I 0.665 0.005 146.177 0.000 0.665 0.501 AGR_II 0.677 0.004 177.939 0.000 0.677 0.596 AGR_III 0.427 0.004 94.987 0.000 0.427 0.336 AGR_IV 0.787 0.004 212.670 0.000 0.787 0.698 AGR_V 0.797 0.004 209.239 0.000 0.797 0.688 EMO =~ EST_I 1.077 0.004 264.435 0.000 1.077 0.819 EST_II 0.722 0.004 181.993 0.000 0.722 0.589 EST_III 0.783 0.004 219.824 0.000 0.783 0.695 EST_IV 0.439 0.004 102.545 0.000 0.439 0.351 EST_V 0.561 0.004 133.401 0.000 0.561 0.448 OPEN =~ OPN_I 0.534 0.004 129.802 0.000 0.534 0.461 OPN_II 0.796 0.004 206.772 0.000 0.796 0.725 OPN_III 0.450 0.004 114.457 0.000 0.450 0.410 OPN_IV 0.702 0.004 187.002 0.000 0.702 0.653 OPN_V 0.441 0.004 125.096 0.000 0.441 0.445 CON =~ CSN_I 0.687 0.004 168.047 0.000 0.687 0.583 CSN_II 0.797 0.005 167.636 0.000 0.797 0.582 CSN_III 0.358 0.004 94.671 0.000 0.358 0.343 CSN_IV 0.808 0.004 190.284 0.000 0.808 0.655 CSN_V 0.782 0.004 177.832 0.000 0.782 0.615 Covariances: Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all EXTRA ~~ AGREE 0.388 0.004 108.706 0.000 0.388 0.388 EMO 0.247 0.004 66.694 0.000 0.247 0.247 OPEN 0.134 0.004 33.270 0.000 0.134 0.134 CON 0.140 0.004 34.647 0.000 0.140 0.140 AGREE ~~ EMO -0.072 0.004 -17.563 0.000 -0.072 -0.072 OPEN 0.189 0.004 44.841 0.000 0.189 0.189 CON 0.120 0.004 28.003 0.000 0.120 0.120 EMO ~~ OPEN 0.178 0.004 43.620 0.000 0.178 0.178 CON 0.241 0.004 60.043 0.000 0.241 0.241 OPEN ~~ CON 0.071 0.004 16.110 0.000 0.071 0.071 Variances: Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all .EXT_I 0.897 0.005 189.010 0.000 0.897 0.561 .EXT_II 0.892 0.005 183.014 0.000 0.892 0.522 .EXT_III 0.751 0.004 180.594 0.000 0.751 0.508 .EXT_IV 0.719 0.004 177.781 0.000 0.719 0.493 .EXT_V 0.725 0.004 167.795 0.000 0.725 0.444 .AGR_I 1.323 0.007 198.755 0.000 1.323 0.749 .AGR_II 0.835 0.005 182.157 0.000 0.835 0.645 .AGR_III 1.428 0.007 214.249 0.000 1.428 0.887 .AGR_IV 0.650 0.004 150.839 0.000 0.650 0.512 .AGR_V 0.705 0.005 154.735 0.000 0.705 0.526 .EST_I 0.570 0.005 104.019 0.000 0.570 0.330 .EST_II 0.982 0.005 191.825 0.000 0.982 0.653 .EST_III 0.657 0.004 163.529 0.000 0.657 0.517 .EST_IV 1.373 0.006 215.662 0.000 1.373 0.877 .EST_V 1.256 0.006 209.221 0.000 1.256 0.800 .OPN_I 1.059 0.005 199.927 0.000 1.059 0.788 .OPN_II 0.572 0.005 123.242 0.000 0.572 0.475 .OPN_III 1.003 0.005 206.019 0.000 1.003 0.832 .OPN_IV 0.663 0.004 153.580 0.000 0.663 0.574 .OPN_V 0.788 0.004 201.943 0.000 0.788 0.802 .CSN_I 0.914 0.005 177.508 0.000 0.914 0.660 .CSN_II 1.240 0.007 177.830 0.000 1.240 0.661 .CSN_III 0.960 0.005 211.978 0.000 0.960 0.882 .CSN_IV 0.869 0.006 156.051 0.000 0.869 0.571 .CSN_V 1.007 0.006 169.093 0.000 1.007 0.622 EXTRA 1.000 1.000 1.000 AGREE 1.000 1.000 1.000 EMO 1.000 1.000 1.000 OPEN 1.000 1.000 1.000 CON 1.000 1.000 1.000 Then, we can also display a plot of the CFA results.\nlavaanPlot( model = original_CFA, node_options = list(shape = \u0026#34;box\u0026#34;, fontname = \u0026#34;Helvetica\u0026#34;), edge_options = list(color = \u0026#34;grey\u0026#34;), coefs = TRUE, graph_options = list(layout = \u0026#34;circo\u0026#34;) ) Now, we run factor analysis again on the synthesized data.\nsynthetic_CFA \u0026lt;- cfa(model = syntax, data = syndat, std.lv = TRUE) fitMeasures(synthetic_CFA, fit.measures = c(\u0026#34;cfi\u0026#34;,\u0026#34;srmr\u0026#34;, \u0026#34;rmsea\u0026#34;)) cfi srmr rmsea 0.811 0.066 0.067 Check the CFA (on synthetic data) results in detail. lavaan 0.6-10 ended normally after 18 iterations Estimator ML Optimization method NLMINB Number of model parameters 60 Number of observations 100000 Model Test User Model: Test statistic 116919.160 Degrees of freedom 265 P-value (Chi-square) 0.000 Model Test Baseline Model: Test statistic 625458.621 Degrees of freedom 300 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.813 Tucker-Lewis Index (TLI) 0.789 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3741187.475 Loglikelihood unrestricted model (H1) -3682727.895 Akaike (AIC) 7482494.950 Bayesian (BIC) 7483065.725 Sample-size adjusted Bayesian (BIC) 7482875.043 Root Mean Square Error of Approximation: RMSEA 0.066 90 Percent confidence interval - lower 0.066 90 Percent confidence interval - upper 0.067 P-value RMSEA \u0026lt;= 0.05 0.000 Standardized Root Mean Square Residual: SRMR 0.066 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all EXTRA =~ EXT_I 0.836 0.004 217.009 0.000 0.836 0.660 EXT_II 0.905 0.004 230.084 0.000 0.905 0.691 EXT_III 0.853 0.004 235.714 0.000 0.853 0.704 EXT_IV 0.858 0.004 239.002 0.000 0.858 0.711 EXT_V 0.959 0.004 256.354 0.000 0.959 0.750 AGREE =~ AGR_I 0.669 0.005 147.141 0.000 0.669 0.503 AGR_II 0.674 0.004 177.811 0.000 0.674 0.594 AGR_III 0.433 0.004 96.378 0.000 0.433 0.341 AGR_IV 0.788 0.004 213.202 0.000 0.788 0.699 AGR_V 0.796 0.004 210.257 0.000 0.796 0.690 EMO =~ EST_I 1.081 0.004 264.490 0.000 1.081 0.819 EST_II 0.723 0.004 182.167 0.000 0.723 0.589 EST_III 0.786 0.004 220.893 0.000 0.786 0.698 EST_IV 0.438 0.004 102.049 0.000 0.438 0.349 EST_V 0.557 0.004 132.607 0.000 0.557 0.445 OPEN =~ OPN_I 0.535 0.004 129.758 0.000 0.535 0.460 OPN_II 0.794 0.004 206.900 0.000 0.794 0.723 OPN_III 0.457 0.004 116.120 0.000 0.457 0.415 OPN_IV 0.708 0.004 189.101 0.000 0.708 0.659 OPN_V 0.440 0.004 124.628 0.000 0.440 0.443 CON =~ CSN_I 0.693 0.004 169.387 0.000 0.693 0.587 CSN_II 0.797 0.005 168.217 0.000 0.797 0.583 CSN_III 0.357 0.004 94.143 0.000 0.357 0.341 CSN_IV 0.808 0.004 190.368 0.000 0.808 0.654 CSN_V 0.789 0.004 179.113 0.000 0.789 0.618 Covariances: Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all EXTRA ~~ AGREE 0.395 0.004 111.237 0.000 0.395 0.395 EMO 0.246 0.004 66.386 0.000 0.246 0.246 OPEN 0.137 0.004 34.000 0.000 0.137 0.137 CON 0.149 0.004 37.068 0.000 0.149 0.149 AGREE ~~ EMO -0.071 0.004 -17.432 0.000 -0.071 -0.071 OPEN 0.198 0.004 47.030 0.000 0.198 0.198 CON 0.126 0.004 29.356 0.000 0.126 0.126 EMO ~~ OPEN 0.171 0.004 41.849 0.000 0.171 0.171 CON 0.237 0.004 58.889 0.000 0.237 0.237 OPEN ~~ CON 0.070 0.004 15.891 0.000 0.070 0.070 Variances: Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all .EXT_I 0.905 0.005 189.674 0.000 0.905 0.564 .EXT_II 0.898 0.005 183.356 0.000 0.898 0.523 .EXT_III 0.741 0.004 180.280 0.000 0.741 0.505 .EXT_IV 0.720 0.004 178.370 0.000 0.720 0.494 .EXT_V 0.716 0.004 166.717 0.000 0.716 0.438 .AGR_I 1.321 0.007 198.630 0.000 1.321 0.747 .AGR_II 0.831 0.005 182.753 0.000 0.831 0.647 .AGR_III 1.426 0.007 214.048 0.000 1.426 0.884 .AGR_IV 0.651 0.004 151.337 0.000 0.651 0.512 .AGR_V 0.698 0.005 154.653 0.000 0.698 0.524 .EST_I 0.575 0.006 104.022 0.000 0.575 0.330 .EST_II 0.983 0.005 191.841 0.000 0.983 0.653 .EST_III 0.652 0.004 162.567 0.000 0.652 0.513 .EST_IV 1.377 0.006 215.764 0.000 1.377 0.878 .EST_V 1.255 0.006 209.457 0.000 1.255 0.802 .OPN_I 1.067 0.005 200.191 0.000 1.067 0.788 .OPN_II 0.575 0.005 124.617 0.000 0.575 0.477 .OPN_III 1.003 0.005 205.597 0.000 1.003 0.828 .OPN_IV 0.653 0.004 151.889 0.000 0.653 0.566 .OPN_V 0.792 0.004 202.352 0.000 0.792 0.804 .CSN_I 0.913 0.005 176.974 0.000 0.913 0.656 .CSN_II 1.234 0.007 177.891 0.000 1.234 0.660 .CSN_III 0.966 0.005 212.219 0.000 0.966 0.884 .CSN_IV 0.873 0.006 156.813 0.000 0.873 0.572 .CSN_V 1.009 0.006 168.548 0.000 1.009 0.618 EXTRA 1.000 1.000 1.000 AGREE 1.000 1.000 1.000 EMO 1.000 1.000 1.000 OPEN 1.000 1.000 1.000 CON 1.000 1.000 1.000 And we show the plot as well\nlavaanPlot( model = synthetic_CFA, node_options = list(shape = \u0026#34;box\u0026#34;, fontname = \u0026#34;Helvetica\u0026#34;), edge_options = list(color = \u0026#34;grey\u0026#34;), coefs = TRUE, graph_options = list(layout = \u0026#34;circo\u0026#34;) ) ","date":"December 15, 2022","image":"http://odissei-soda.nl/images/tutorial-3/tutorial3_header_hudfeb7f39f2da94fb9b40217771ccd9c4_772114_650x0_resize_box_3.png","permalink":"/tutorials/post-3/","title":"Reproducible science using Statistics Netherlands microdata"},{"categories":null,"contents":"ArtScraper is a Python library to download images and metadata for artworks available on WikiArt and Google Arts \u0026amp; Culture.\nInstallation pip install https://github.com/sodascience/artscraper.git Downloading images from WikiArt To download data from WikiArt it is necessary to obtain free API keys.\nOnce you have the API keys, you can simply run the code below to download the images and metadata of the three artworks of Aleksandra Ekster.\n# artworks to scrape some_links = [ \u0026#34;https://www.wikiart.org/en/aleksandra-ekster/women-s-costume-1918\u0026#34;, \u0026#34;https://www.wikiart.org/en/aleksandra-ekster/still-life-1913\u0026#34;, \u0026#34;https://www.wikiart.org/en/aleksandra-ekster/view-of-paris-1912\u0026#34; ] # download images and metadata to the folder \u0026#34;data\u0026#34; with WikiArtScraper(output_dir=\u0026#34;data\u0026#34;) as scraper: for url in some_links: scraper.load_link(url) scraper.save_metadata() scraper.save_image() Downloading images from Google Art \u0026amp; Culture To download data from Google Art \u0026amp; Culture you need to download Firefox and geckodriver. The installation instructions can be found in our GitHub repository.\nOnce you have Firefox and geckodriver, you can simply run the code below to download artworks. You are not allowed to share or publish the images. Use them only for research.\n# artworks to scrape some_links = [ \u0026#34;https://artsandculture.google.com/asset/helena-hunter-fairytales/dwFMypq0ZSiq6w\u0026#34;, \u0026#34;https://artsandculture.google.com/asset/erina-takahashi-and-isaac-hernandez-in-fantastic-beings-laurent-liotardo/MQEhgoWpWJUd_w\u0026#34;, \u0026#34;https://artsandculture.google.com/asset/rinaldo-roberto-masotti/swG7r2rgfvPOFQ\u0026#34; ] # If you are on Windows, you can download geckodriver, place it in your directory, # and use the argument geckodriver_path=\u0026#34;geckodriver.exe\u0026#34; with GoogleArtScraper(\u0026#34;data\u0026#34;) as scraper: for url in some_links: scraper.load_link(url) scraper.save_metadata() scraper.save_image() You can find more examples here\nDo you want to know more about this library? Check our GitHub repository\nAre you using it for academic work? Please cite our package:\nSchram, Raoul, Garcia-Bernardo, Javier, van Kesteren, Erik-Jan, de Bruin, Jonathan, \u0026amp; Stamkou, Eftychia. (2022). ArtScraper: A Python library to scrape online artworks (0.1.1). Zenodo. https://doi.org/10.5281/zenodo.7129975 ","date":"October 4, 2022","image":"http://odissei-soda.nl/images/tutorial-2/nantenbo_header_hu48d98726654ff846597d2fe25a58192f_52421_650x0_resize_q100_box.jpg","permalink":"/tutorials/post-2/","title":"ArtScraper: A Python library to scrape online artworks"},{"categories":null,"contents":"With the increasing popularity of open science practices, it is now more and more common to openly share code along with more traditional scientific objects such as papers. But what are the best ways to create an understandable, openly accessible, findable, citable, and stable archive of your code? In this post, we look at what you need to do to prepare your code folder and then how to upload it to Zenodo.\nPrepare your code folder To make code available, you will be uploading it to the internet as a single folder. The code you will upload will be openly accessible, and it will stay that way indefinitely. Therefore, it is necessary that you prepare your code folder (also called a “repository”) for publication. This requires time and effort, and for every project the requirements are different. Think about the following checklist:\nMust-haves Make a logical, understandable folder structure. For example, for a research project with data processing, visualization, and analysis I like the following structure: my_project/\u0026lt;br/\u0026gt; ├─ raw_data/\u0026lt;br/\u0026gt; │ ├─ questionnaire_data.csv\u0026lt;br/\u0026gt; ├─ processed_data/\u0026lt;br/\u0026gt; │ ├─ questionnaire_processed.rds\u0026lt;br/\u0026gt; │ ├─ analysis_object.rds\u0026lt;br/\u0026gt; ├─ img/\u0026lt;br/\u0026gt; │ ├─ plot.png\u0026lt;br/\u0026gt; ├─ 01_load_and_process_data.R\u0026lt;br/\u0026gt; ├─ 02_create_visualisations.R\u0026lt;br/\u0026gt; ├─ 03_main_analysis.R\u0026lt;br/\u0026gt; ├─ 04_output_results.R\u0026lt;br/\u0026gt; ├─ my_project.Rproj\u0026lt;br/\u0026gt; ├─ readme.md Make sure no privacy-sensitive information is leaked. Remove non-shareable data objects (raw and processed!), passwords hardcoded in your scripts, comments containing private information, and so on. Create a legible readme file in the folder that describes what the code does, where to find which parts of the code, and what needs to be done to run the code. You can choose how elaborate to make this! It could be a simple text file, a word document, a pdf, or a markdown document with images describing the structure. It is best if someone who does not know the project can understand the entire folder based on the readme – this includes yourself in a few years from now! Strong recommendations Reformat the code so that it is portable and easily reproducible. This means that when someone else downloads the folder, they do not need to change the code to run it. For example this means that you do not read data with absolute paths (e.g., C:/my_name/Documents/PhD/projects/project_title/raw_data/questionnaire_data.csv) on your computer, but only to relative paths on the project (e.g., raw_data/questionnaire_data.csv). For example, if you use the R programming language it is good practice to use an RStudio project. Format your code so that it is legible by others. Write informative comments, split up your scripts in logical chunks, and use a consistent style (for R I like the tidyverse style) Nice to have Record the software packages that you used to run the projects, including their versions. If a package gets updated, your code may no longer run! Your package manager may already do this, e.g., for python you can use pip freeze \u0026gt; requirements.txt. In R, you can use the renv package for this. If you have privacy-sensitive data, it may still be possible to create a synthetic or fake version of this data for others to run the code on. This ensures maximum reproducibility. Compressing the code folder The last step before uploading the code repository to Zenodo is to compress the folder. This can be done in Windows 11 by right-clicking the folder and pressing “compress to zip file”. It’s a good idea to go into the compressed folder afterwards, and checking if everything is there and also removing any unnecessary files (such as .Rhistory files for R).\nFigure 1: Zipping the code folder.\rAfter compressing, your code repository is now ready to be uploaded!\nUploading to Zenodo Zenodo is a website where you can upload any kind of research object: papers, code, datasets, questionnaires, presentations, and much more. After uploading, Zenodo will create a page containing your research object and metadata about the object, such as publication date, author, and keywords. In the figure below you can see an example of a code repository uploaded to Zenodo.\nFigure 2: A code repository uploaded to the Zenodo website. See https://zenodo.org/record/6504837\rOne of the key features of Zenodo is that you can get a Digital Object Identifier (DOI) for the objects you upload, making your research objects persistent and easy to find and cite. For example, in APA style I could cite the code as follows:\nvan Kesteren, Erik-Jan. (2022). My project (v1.2). Zenodo. https://doi.org/10.5281/zenodo.6504837\nZenodo itself is fully open source, hosted by CERN, and funded by the European Commission. These are exactly the kinds of conditions which make it likely to last for a long time! Hence, it is an excellent choice for uploading our code. So let’s get started!\nCreate an account To upload anything to Zenodo, you need an account. If you already have an ORCID or a GitHub account, then you can link these immediately to your Zenodo login. I do recommend doing so as it will make it easy to link these services and use them together.\nFigure 3: Zenodo sign-up page. See https://zenodo.org/signup/\rStart a new upload When you click the “upload” button, you will get a page where you can upload your files, determine the type of the upload, and create metadata for the research object. Now zip your prepared code folder and drag it to the upload window!\nFigure 4: Uploading a zipped folder to the Zenodo website.\rFill out the metadata One of the first options you need to specify is the “upload type”. For code repositories, you can choose the “software” option. The remaining metadata is relatively simple to fill out (such as author and institution). However, one category to pay attention to is the license: by default the CC-BY-4.0 license is selected. For a short overview of what this means, see the creative commons website: https://creativecommons.org/licenses/by/4.0/. You can opt for a different license by including a file called LICENSE in your repository.\nFigure 5: Selecting the \u0026lsquo;software\u0026rsquo; option for upload type.\rPublish! The last step is to click “publish”. Your research code is now findable, citable, understandable, reproducible, and archived until the end of times! You can now show it to all your colleagues and easily cite it in your manuscript. If you get feedback and you want to change your code, you can also upload a new version of the same project on the Zenodo website.\nConclusion In this post, I described a checklist for preparing your code folder for publication with a focus on understandability, and I have described one way in which you can upload your prepared code repository to an open access archive. Zenodo is an easy, dependable and well-built option, but of course there are many alternatives, such as hosting it on your own website, using the Open Science Framework, GitHub, or using a publisher’s website; each has its own advantages and disadvantages.\n","date":"September 5, 2022","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/tutorials/post-1/","title":"How to share your research code"},{"categories":null,"contents":"","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/principles/","title":"Principles"},{"categories":null,"contents":"","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/projects/","title":"Projects"},{"categories":null,"contents":"","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/team/","title":"The SoDa Team"},{"categories":null,"contents":"During your time as a trainee:\nyou will spend between 3-8 months full-time* working on a social science research project. you can propose your own project, based on your interests. you are a member of the SoDa team at the Methodology \u0026amp; Statistics department of Utrecht University. you will get a salary during this time, paid for by the team. one of the senior team members will be your mentor. To apply, you have to submit a short proposal for your project, together with a substantive supervisor. We are looking for projects in the social sciences for which a computational or data-related problem needs to be solved. The traineeship is offered in a form of a rolling call for proposals which is open throughout the academic year and subject to availability.\n* part-time possible, but the project should be your main priority.\n","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/traineeship/","title":"Traineeship"}]