







































































































[{"categories":null,"contents":"In this tutorial we present Geoflow, a newly created tool designed to visualize international flows in an interactive way. The tool is free and open-source, and can be accessed here. It is designed to visualize any international flows like, for instance, cash or migration flows. Since it\u0026rsquo;s easier to understand its capabilities by using them, we\u0026rsquo;ll start showing a couple of examples of what Geoflow can do. After that, we briefly explain how to upload your own dataset and visualize it using the tool.\nWhat Geoflow can do For these examples, we use the included demo dataset showing investments in fossil companies across countries. Figure 1 shows the top 10 investments in fossil fuel companies from China. The visualization is straightforward: the arrows indicate in which countries these investments are located. By placing the cursor on top of Bermudas, its arrow gets highlighted, showing the flow strength (weight), as well as the inflow and outflow country. Figure 2 is a barplot that shows to which countries most investments go to: in this case, it is mainly Singapore, followed by the Netherlands.\nFigure 1. Map ok top 10 investments in fossil fuel companies from China\rFigure 2. Barplot of 10 investments in fossil fuel companies from China\rIn general, in the tool we can select the source and target countries for which we want to see the flows. There is also an option to select source or target for a country, which is useful when we want to focus on one country: for example, selecting all flows into the Netherlands or from the Netherlands. Besides, we can select the number of flows to visualize in the upper part. Lastly, we can select whether we want to visualize the inflow or the outflow. This last option changes the colouring of the countries (colouring either the inflow or outflow countries), and it also changes the barplot visualization.\nFigure 3. Some configuration options for the visualization\rHow to use Geoflow for your visualizations It is very straightforward to use Geoflow for your own visualizations. You only need to upload a .csv dataset to the app with the following columns:\nSource: the source of the flow, written in ISO2 format. Target: the target of the flow, also in ISO2 format Weight: strength of the flow (e.g., the number of migrants, or the financial revenue) Additionally, you can add:\nYear: If year is present, a new visualization on the bottom right panel shows a time series for the flows. You can see it in Figure 4 below. Other columns: They will be interpreted as categorical variables. This allows you to split the flows into categories, as shown in Figure 5 It is a requirement that the data format is .csv and the name of the variables are source, target, weight and (if included) year, so note you might have to reformat your data to use the tool.\nFigure 4. How the time series plot look like, with Germany highlighted\rFigure 5. Map of China top 10 fossil fuel investments, showing only investments in Manufacturing\rConclusion In this tutorial we have shown you how to use Geoflow for your own visualizations. We have shown what the Geoflow capabilities are, and how to upload your data to use it. We hope you\u0026rsquo;ve find it inspiring!\nGeoflow is open-source and has been developed by Peter Kok, Javier García Bernardo and mbabic332. If you use the tool, you can cite this Zenodo repo, with a DOI. If you wish to expand on Geoflow, you can check the source code here, and contribute or build on it. The tool is written using JavaScript.\n","date":"September 29, 2023","image":"http://odissei-soda.nl/images/tutorial-7/geoflow-miniature_hu6dbb4503ac6d6ce6a020e99dbb85449c_282734_650x0_resize_box_3.png","permalink":"/tutorials/post-7/","title":"Visualizing international flows with Geoflow visualizer"},{"categories":null,"contents":"One common issue we encounter in helping researchers work with the housing register data of Statistics Netherlands is its transactional nature: each row in the housing register table contains data on when someone registered and deregistered at an address (more info in Dutch here).\nIn this post, we show how to use this transactional data to perform one of the most common transformations we see: what part of a certain time interval (e.g, the entire year 2021 or January 1999) did the people I’m interested in live in the Netherlands? To solve this issue, we will use time interval objects, as implemented in the package {lubridate} which is part of the {tidyverse} since version 2.0.0.\nlibrary(tidyverse) The data Obviously, we cannot share actual Statistics Netherlands microdata here, so we first generate some tables that capture the gist of the data structure. First, let’s generate some basic person identifiers and some info about each person:\nCode (person_df \u0026lt;- tibble( person_id = factor(c(\u0026#34;A10232\u0026#34;, \u0026#34;A39211\u0026#34;, \u0026#34;A28183\u0026#34;, \u0026#34;A10124\u0026#34;)), firstname = c(\u0026#34;Aron\u0026#34;, \u0026#34;Beth\u0026#34;, \u0026#34;Carol\u0026#34;, \u0026#34;Dave\u0026#34;), income_avg = c(14001, 45304, 110123, 43078) )) # A tibble: 4 × 3 person_id firstname income_avg \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; 1 A10232 Aron 14001 2 A39211 Beth 45304 3 A28183 Carol 110123 4 A10124 Dave 43078 Then, we create a small example of housing transaction register data. In this data, for any period where a person is not registered to a house, they are assumed to live abroad (because everyone in the Netherlands is required to be registered at an address).\nCode (house_df \u0026lt;- tibble( person_id = factor(c(\u0026#34;A10232\u0026#34;, \u0026#34;A10232\u0026#34;, \u0026#34;A10232\u0026#34;, \u0026#34;A39211\u0026#34;, \u0026#34;A39211\u0026#34;, \u0026#34;A28183\u0026#34;, \u0026#34;A28183\u0026#34;, \u0026#34;A10124\u0026#34;)), house_id = factor(c(\u0026#34;H1200E\u0026#34;, \u0026#34;H1243D\u0026#34;, \u0026#34;H3432B\u0026#34;, \u0026#34;HA7382\u0026#34;, \u0026#34;H53621\u0026#34;, \u0026#34;HC39EF\u0026#34;, \u0026#34;HA3A01\u0026#34;, \u0026#34;H222BA\u0026#34;)), start_date = ymd(c(\u0026#34;20200101\u0026#34;, \u0026#34;20200112\u0026#34;, \u0026#34;20211120\u0026#34;, \u0026#34;19800101\u0026#34;, \u0026#34;19900101\u0026#34;, \u0026#34;20170303\u0026#34;, \u0026#34;20190202\u0026#34;, \u0026#34;19931023\u0026#34;)), end_date = ymd(c(\u0026#34;20200112\u0026#34;, \u0026#34;20211120\u0026#34;, \u0026#34;20230720\u0026#34;, \u0026#34;19891231\u0026#34;, \u0026#34;20170102\u0026#34;, \u0026#34;20180720\u0026#34;, \u0026#34;20230720\u0026#34;, \u0026#34;20230720\u0026#34;)) )) # A tibble: 8 × 4 person_id house_id start_date end_date \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; 1 A10232 H1200E 2020-01-01 2020-01-12 2 A10232 H1243D 2020-01-12 2021-11-20 3 A10232 H3432B 2021-11-20 2023-07-20 4 A39211 HA7382 1980-01-01 1989-12-31 5 A39211 H53621 1990-01-01 2017-01-02 6 A28183 HC39EF 2017-03-03 2018-07-20 7 A28183 HA3A01 2019-02-02 2023-07-20 8 A10124 H222BA 1993-10-23 2023-07-20 Interval objects! Notice how each transaction in the housing data has a start and end date, indicating when someone registered and deregistered at an address. A natural representation of this information is as a single object: a time interval. The package {lubridate} has support for specific interval objects, and several operations on intervals:\ncomputing the length of an interval with int_length() computing whether two intervals overlap with int_overlap() and much more\u0026hellip; as you can see here So let’s transform these start and end columns into a single interval column!\nhouse_df \u0026lt;- house_df |\u0026gt; mutate( # create the interval int = interval(start_date, end_date), # drop the start/end columns .keep = \u0026#34;unused\u0026#34; ) house_df # A tibble: 8 × 3 person_id house_id int \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;Interval\u0026gt; 1 A10232 H1200E 2020-01-01 UTC--2020-01-12 UTC 2 A10232 H1243D 2020-01-12 UTC--2021-11-20 UTC 3 A10232 H3432B 2021-11-20 UTC--2023-07-20 UTC 4 A39211 HA7382 1980-01-01 UTC--1989-12-31 UTC 5 A39211 H53621 1990-01-01 UTC--2017-01-02 UTC 6 A28183 HC39EF 2017-03-03 UTC--2018-07-20 UTC 7 A28183 HA3A01 2019-02-02 UTC--2023-07-20 UTC 8 A10124 H222BA 1993-10-23 UTC--2023-07-20 UTC We will want to compare this interval with a reference interval to compute the proportion of time that a person lived in the Netherlands within the reference interval. Therefore, we quickly define a new interval operation which truncates an interval to a reference interval. Don’t worry too much about it for now, we will use it later. Do notice that we’re always using the int_*() functions defined by {lubridate} to interact with the interval objects.\n# utility function to truncate an interval object to limits (also vectorized so it works in mutate()) int_truncate \u0026lt;- function(int, int_limits) { int_start(int) \u0026lt;- pmax(int_start(int), int_start(int_limits)) int_end(int) \u0026lt;- pmin(int_end(int), int_end(int_limits)) return(int) } Computing the proportion in the Netherlands The next step is to define a function that computes for each person a proportion overlap for a reference interval. By creating a function, it will be easy later to do the same operation for different intervals (e.g., different reference years) to work with the rich nature of the Statistics Netherlands microdata. To compute this table, we make extensive use of the {tidyverse}, with verbs like filter(), mutate(), and summarize(). If you want to know more about these, take a look at the {dplyr} documentation (but of course you can also use your own flavour of data processing, such as {data.table} or base R).\n# function to compute overlap proportion per person proportion_tab \u0026lt;- function(housing_data, reference_interval) { # start with the housing data housing_data |\u0026gt; # only retain overlapping rows, this makes the following # operations more efficient by only computing what we need filter(int_overlaps(int, reference_interval)) |\u0026gt; # then, actually compute the overlap of the intervals mutate( # use our earlier truncate function int_tr = int_truncate(int, reference_interval), # then, it\u0026#39;s simple to compute the overlap proportion prop = int_length(int_tr) / int_length(reference_interval) ) |\u0026gt; # combine different intervals per person summarize(prop_in_nl = sum(prop), .by = person_id) } Now we’ve defined this function, let’s try it out for a specific year such as 2017!\nint_2017 \u0026lt;- interval(ymd(\u0026#34;20170101\u0026#34;), ymd(\u0026#34;20171231\u0026#34;)) prop_2017 \u0026lt;- proportion_tab(house_df, int_2017) prop_2017 # A tibble: 3 × 2 person_id prop_in_nl \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; 1 A39211 0.00275 2 A28183 0.832 3 A10124 1 Now we’ve computed this proportion, notice that we only have three people. This means that the other person was living abroad in that time, with a proportion in the Netherlands of 0. To nicely display this information, we can join the proportion table with the original person dataset and replace the NA values in the proportion column with 0.\nleft_join(person_df, prop_2017, by = \u0026#34;person_id\u0026#34;) |\u0026gt; mutate(prop_in_nl = replace_na(prop_in_nl, 0)) # A tibble: 4 × 4 person_id firstname income_avg prop_in_nl \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 A10232 Aron 14001 0 2 A39211 Beth 45304 0.00275 3 A28183 Carol 110123 0.832 4 A10124 Dave 43078 1 Success! We now have a dataset for each person with the proportion of time they lived in the Netherlands in 2017. If you look at the original housing dataset, you may see the following patterns reflected in the proportion:\nAron indeed did not live in the Netherlands at this time. Beth moved away on January 2nd, 2017. Carol moved into the Netherlands on March 3rd, 2017 and remained there until 2018 Dave lived in the Netherlands this entire time. Conclusion In this post, we used interval objects and operations from the {lubridate} package to wrangle transactional housing data into a proportion of time spent living in the Netherlands. The advantage of using this package and its functions is that any particularities with timezones, date comparison, and leap years are automatically dealt with so that we could focus on the end result rather than the details.\nIf you are doing similar work and you have a different method, let us know! In addition, if you have further questions about working with Statistics Netherlands microdata or other complex or large social science datasets, do not hesitate to contact us on our website: https://odissei-soda.nl.\nBonus appendix: multiple time intervals Because we created a function that takes in the transaction data and a reference interval, we can do the same thing for multiple time intervals (e.g., years) and combine the data together in one wide or long dataset. This is one way to do this:\nlibrary(glue) # for easy string manipulation # initialize an empty dataframe with all our columns nl_prop \u0026lt;- tibble(person_id = factor(), prop_in_nl = double(), yr = integer()) # then loop over the years of interest for (yr in 2017L:2022L) { # construct reference interval for this year ref_int \u0026lt;- interval(ymd(glue(\u0026#34;{yr}0101\u0026#34;)), ymd(glue(\u0026#34;{yr}1231\u0026#34;))) # compute the proportion table for this year nl_prop_yr \u0026lt;- proportion_tab(house_df, ref_int) |\u0026gt; mutate(yr = yr) # append this year to the dataframe nl_prop \u0026lt;- bind_rows(nl_prop, nl_prop_yr) } # we can pivot it to a wide format nl_prop_wide \u0026lt;- nl_prop |\u0026gt; pivot_wider( names_from = yr, names_prefix = \u0026#34;nl_prop_\u0026#34;, values_from = prop_in_nl ) # and join it with the original person data, replacing NAs with 0 again person_df |\u0026gt; left_join(nl_prop_wide, by = \u0026#34;person_id\u0026#34;) |\u0026gt; mutate(across(starts_with(\u0026#34;nl_prop_\u0026#34;), \\(p) replace_na(p, 0))) # A tibble: 4 × 9 person_id firstname income_avg nl_prop_2017 nl_prop_2018 nl_prop_2019 \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 A10232 Aron 14001 0 0 0 2 A39211 Beth 45304 0.00275 0 0 3 A28183 Carol 110123 0.832 0.549 0.912 4 A10124 Dave 43078 1 1 1 # ℹ 3 more variables: nl_prop_2020 \u0026lt;dbl\u0026gt;, nl_prop_2021 \u0026lt;dbl\u0026gt;, # nl_prop_2022 \u0026lt;dbl\u0026gt; ","date":"September 29, 2023","image":"http://odissei-soda.nl/images/tutorial-5/lubridate_1_hu9ab8adf199cf1b31e00f5b6813db3886_430902_650x0_resize_box_3.png","permalink":"/tutorials/post-5/","title":"Wrangling interval data using lubridate"},{"categories":null,"contents":"These days, our online presence leaves traces of our behavior everywhere. There is data of what we do and say in platforms such as WhatsApp, Instagram, online stores, and many others. Of course, this so-called ‘digital trace data’ is of interest for social scientists: new, rich, enormous datasets that can be used to describe and understand our social world. However, this data is commonly owned by private companies. How can social scientists access and make sense of this data?\nIn this tutorial, we use data donation and the Port software to get access to WhatsApp group-chat data in a way that completely preserves privacy of research participants. Our goal is to show a small peek of what can be achieved with these methods. If you have an idea for your own research that entails collecting digital trace data, don’t hesitate to contact us! We can help you think about data acquisition, analysis and more.\nWith data donation, it is possible to collect data about any online platform: under the General Data Protection Regulation (EU law), companies are required to provide their data to any citizen that requests it. This data is available in so-called Data Download Packages (DDP’s), which are rather cumbersome to work with and contain personal information. Therefore, the Port software processes these DDP’s so that the data is in a format ready for analysis, while completely guaranteeing privacy of the respondents. The only thing research participants have to do is request their DDP’s, see which information they are sharing and consent to sharing it.\nSince we do not dive in with a lot of detail, we refer to Port\u0026rsquo;s github for more details on how to get started with your own project. There you can find a full guide to install and use Port, examples of past studies done with it, a tutorial for creating your own data donation workflow, and more. You can also read more about data donation in general here and here.\nAn application with WhatsApp data In this example, we extract some basic information from WhatsApp group chats, such as how many messages links, locations, and pictures were shared, as well as which person in the group the participant responded most to.\nNote that this is the only information we want to collect from the participants of the study, not the whole group chat file!\nThe first step in creating a DDP processing script is to obtain an example DDP and examine it. This example DDP can be, for example, your own DDP requested from WhatsApp. Usually, platforms provide a (compressed) folder with many different files; i.e., data in a format that is not ready to use. Once uncompressed, a WhatsApp group chat file could look like this:\n[16/03/2022, 15:10:17] Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more. [16/03/2022, 15:20:25] person1: Hi shiva! [16/03/2022, 15:25:38] person2: Hi 👋 [16/03/2022, 15:26:48] person3: Hoi! [16/03/2022, 18:39:29] person2: https://youtu.be/KBmUTY6mK_E [16/03/2022, 18:35:51] person1: ‎Location: https://maps.google.com/?q=52.089451,5.108469 [20/03/2022, 20:08:51] person4: I’m about to generate some very random messages so that I can make some screenshots for the explanation to participants [24/03/2022, 20:19:38] person1: @user3 if you remove your Profile picture for a moment I will redo the screenshots 😁 [26/03/2022, 18:52:15] person2: Well done Utrecht 😁 [14/07/2020, 22:05:54] person4: 👍Bedankt As part of a collaboration with data donation researchers using Port, we wrote a Python script1 to convert this into the information we need. The main script is available here; in short, it does the following:\nseparate the header and the message itself parse the date and time information in each message remove unneeded information such as alert notifications anonymize usernames convert the extracted information to a nice data frame format to show the participant for consent One big problem we had to overcome is that messages and alert notifications cannot be identified in the same way (i.e., using the same regular expression) on every device. Through trial-and-error, we tailored the steps to work with every operating system, language, and device required for this study. Indeed, if you design a study like this, it is very important to try out your script on many different DDPs from different people and devices. That way you will make sure you have covered possible variation in DPPs before actually starting data collection. This is a process that can take quite a while, so keep this in mind when you want to run a data donation study!\nThe end result In Figure 1 you can see a (fictitious) snippet of the dataset obtained. This is how a dataset in which you combine donations from different users would look like. As can be seen, we have moved from a rather untidy text file to a tidy, directly analysable dataset, where each row corresponds to an user in a given data donation package, and the rest of the columns give information about that user. Particularly, the dataset displays the following information: data donation package id (ddp_id), an anonymized user name (user_name), number of words sent on the groupchat by the user (nwords), date of first and last messages sent on the groupchat by the user (date_first_mess, date_last_mess), number of urls, files and locations sent on the groupchat by the user (respectively, nurls, nfiles, nlocations), and the (other) user that has replied more to that user (replies_from), as well as the user that that user has replied to the most (replies_to).\nTable 1. Snippet of fictitious dataset\nddp_id user_name nwords date_first_mess date_last_mess nurls nfiles nlocations replies_from replies_to 1 User1_1 121 10/08/2023 27/08/2023 0 15 0 User1_2 User1_4 1 User1_2 17 11/08/2023 28/08/2023 3 1 2 User1_1 User1_1 1 User1_3 44 10/08/2023 28/08/2023 9 6 3 User1_2 User1_1 1 User1_4 50 12/08/2023 29/08/2023 0 3 1 User1_3 User1_1 2 User2_1 123 01/05/2022 01/11/2022 2 0 1 User2_2 User2_2 2 User2_2 250 01/05/2022 02/11/2022 0 32 3 User2_3 User2_1 2 User2_3 176 08/07/2022 04/12/2022 6 0 5 User2_2 User2_3 3 User3_1 12 05/06/2023 26/07/2023 12 2 0 User3_1 User3_2 3 User3_2 16 06/06/2023 26/07/2023 17 2 0 User3_2 User3_1 In Figure 2 you can see a screenshot of how the Port software would display the data to be shared (number of words or messages, date stamps…) and ask for consent to the research subjects. As you see, the Port software guarantees that research subjects are aware of what information they are sharing and consent to it. The rest of the DDPs, including sensitive data, is analyzed locally and does not leave the respondents\u0026rsquo; devices.\nFigure 2. How the Port software displays the data to be shared and asks for consent\rConclusion The aim of this post was to illustrate how to use data donation with the software Port to extract online platform data. We illustrated all of this with the extraction of group-chat information from WhatsApp data. The main challenge of this project was to write a robust script that transforms this data into a nice, readily usable format while maintaining privacy. If you want to implement something similar but do not know how or where to start, let us know and we can help!\nThis script uses a deprecated version of Port, but large part of the script can be reused;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"September 8, 2023","image":"http://odissei-soda.nl/images/tutorial-4/whatsapp_header_hu3d03a01dcc18bc5be0e67db3d8d209a6_1708177_650x0_resize_q100_box.jpg","permalink":"/tutorials/post-4/","title":"Collecting online platforms data for science: an example using WhatsApp"},{"categories":null,"contents":"ArtScraper is a Python library to download images and metadata for artworks available on WikiArt and Google Arts \u0026amp; Culture.\nInstallation pip install https://github.com/sodascience/artscraper.git Downloading images from WikiArt To download data from WikiArt it is necessary to obtain free API keys.\nOnce you have the API keys, you can simply run the code below to download the images and metadata of the three artworks of Aleksandra Ekster.\n# artworks to scrape some_links = [ \u0026#34;https://www.wikiart.org/en/aleksandra-ekster/women-s-costume-1918\u0026#34;, \u0026#34;https://www.wikiart.org/en/aleksandra-ekster/still-life-1913\u0026#34;, \u0026#34;https://www.wikiart.org/en/aleksandra-ekster/view-of-paris-1912\u0026#34; ] # download images and metadata to the folder \u0026#34;data\u0026#34; with WikiArtScraper(output_dir=\u0026#34;data\u0026#34;) as scraper: for url in some_links: scraper.load_link(url) scraper.save_metadata() scraper.save_image() Downloading images from Google Art \u0026amp; Culture To download data from Google Art \u0026amp; Culture you need to download Firefox and geckodriver. The installation instructions can be found in our GitHub repository.\nOnce you have Firefox and geckodriver, you can simply run the code below to download artworks. You are not allowed to share or publish the images. Use them only for research.\n# artworks to scrape some_links = [ \u0026#34;https://artsandculture.google.com/asset/helena-hunter-fairytales/dwFMypq0ZSiq6w\u0026#34;, \u0026#34;https://artsandculture.google.com/asset/erina-takahashi-and-isaac-hernandez-in-fantastic-beings-laurent-liotardo/MQEhgoWpWJUd_w\u0026#34;, \u0026#34;https://artsandculture.google.com/asset/rinaldo-roberto-masotti/swG7r2rgfvPOFQ\u0026#34; ] # If you are on Windows, you can download geckodriver, place it in your directory, # and use the argument geckodriver_path=\u0026#34;geckodriver.exe\u0026#34; with GoogleArtScraper(\u0026#34;data\u0026#34;) as scraper: for url in some_links: scraper.load_link(url) scraper.save_metadata() scraper.save_image() You can find more examples here\nDo you want to know more about this library? Check our GitHub repository\nAre you using it for academic work? Please cite our package:\nSchram, Raoul, Garcia-Bernardo, Javier, van Kesteren, Erik-Jan, de Bruin, Jonathan, \u0026amp; Stamkou, Eftychia. (2022). ArtScraper: A Python library to scrape online artworks (0.1.1). Zenodo. https://doi.org/10.5281/zenodo.7129975 ","date":"October 4, 2022","image":"http://odissei-soda.nl/images/tutorial-2/nantenbo_header_hu48d98726654ff846597d2fe25a58192f_52421_650x0_resize_q100_box.jpg","permalink":"/tutorials/post-2/","title":"ArtScraper: A Python library to scrape online artworks"},{"categories":null,"contents":"With the increasing popularity of open science practices, it is now more and more common to openly share code along with more traditional scientific objects such as papers. But what are the best ways to create an understandable, openly accessible, findable, citable, and stable archive of your code? In this post, we look at what you need to do to prepare your code folder and then how to upload it to Zenodo.\nPrepare your code folder To make code available, you will be uploading it to the internet as a single folder. The code you will upload will be openly accessible, and it will stay that way indefinitely. Therefore, it is necessary that you prepare your code folder (also called a “repository”) for publication. This requires time and effort, and for every project the requirements are different. Think about the following checklist:\nMust-haves Make a logical, understandable folder structure. For example, for a research project with data processing, visualization, and analysis I like the following structure: my_project/\u0026lt;br/\u0026gt; ├─ raw_data/\u0026lt;br/\u0026gt; │ ├─ questionnaire_data.csv\u0026lt;br/\u0026gt; ├─ processed_data/\u0026lt;br/\u0026gt; │ ├─ questionnaire_processed.rds\u0026lt;br/\u0026gt; │ ├─ analysis_object.rds\u0026lt;br/\u0026gt; ├─ img/\u0026lt;br/\u0026gt; │ ├─ plot.png\u0026lt;br/\u0026gt; ├─ 01_load_and_process_data.R\u0026lt;br/\u0026gt; ├─ 02_create_visualisations.R\u0026lt;br/\u0026gt; ├─ 03_main_analysis.R\u0026lt;br/\u0026gt; ├─ 04_output_results.R\u0026lt;br/\u0026gt; ├─ my_project.Rproj\u0026lt;br/\u0026gt; ├─ readme.md Make sure no privacy-sensitive information is leaked. Remove non-shareable data objects (raw and processed!), passwords hardcoded in your scripts, comments containing private information, and so on. Create a legible readme file in the folder that describes what the code does, where to find which parts of the code, and what needs to be done to run the code. You can choose how elaborate to make this! It could be a simple text file, a word document, a pdf, or a markdown document with images describing the structure. It is best if someone who does not know the project can understand the entire folder based on the readme – this includes yourself in a few years from now! Strong recommendations Reformat the code so that it is portable and easily reproducible. This means that when someone else downloads the folder, they do not need to change the code to run it. For example this means that you do not read data with absolute paths (e.g., C:/my_name/Documents/PhD/projects/project_title/raw_data/questionnaire_data.csv) on your computer, but only to relative paths on the project (e.g., raw_data/questionnaire_data.csv). For example, if you use the R programming language it is good practice to use an RStudio project. Format your code so that it is legible by others. Write informative comments, split up your scripts in logical chunks, and use a consistent style (for R I like the tidyverse style) Nice to have Record the software packages that you used to run the projects, including their versions. If a package gets updated, your code may no longer run! Your package manager may already do this, e.g., for python you can use pip freeze \u0026gt; requirements.txt. In R, you can use the renv package for this. If you have privacy-sensitive data, it may still be possible to create a synthetic or fake version of this data for others to run the code on. This ensures maximum reproducibility. Compressing the code folder The last step before uploading the code repository to Zenodo is to compress the folder. This can be done in Windows 11 by right-clicking the folder and pressing “compress to zip file”. It’s a good idea to go into the compressed folder afterwards, and checking if everything is there and also removing any unnecessary files (such as .Rhistory files for R).\nFigure 1: Zipping the code folder.\rAfter compressing, your code repository is now ready to be uploaded!\nUploading to Zenodo Zenodo is a website where you can upload any kind of research object: papers, code, datasets, questionnaires, presentations, and much more. After uploading, Zenodo will create a page containing your research object and metadata about the object, such as publication date, author, and keywords. In the figure below you can see an example of a code repository uploaded to Zenodo.\nFigure 2: A code repository uploaded to the Zenodo website. See https://zenodo.org/record/6504837\rOne of the key features of Zenodo is that you can get a Digital Object Identifier (DOI) for the objects you upload, making your research objects persistent and easy to find and cite. For example, in APA style I could cite the code as follows:\nvan Kesteren, Erik-Jan. (2022). My project (v1.2). Zenodo. https://doi.org/10.5281/zenodo.6504837\nZenodo itself is fully open source, hosted by CERN, and funded by the European Commission. These are exactly the kinds of conditions which make it likely to last for a long time! Hence, it is an excellent choice for uploading our code. So let’s get started!\nCreate an account To upload anything to Zenodo, you need an account. If you already have an ORCID or a GitHub account, then you can link these immediately to your Zenodo login. I do recommend doing so as it will make it easy to link these services and use them together.\nFigure 3: Zenodo sign-up page. See https://zenodo.org/signup/\rStart a new upload When you click the “upload” button, you will get a page where you can upload your files, determine the type of the upload, and create metadata for the research object. Now zip your prepared code folder and drag it to the upload window!\nFigure 4: Uploading a zipped folder to the Zenodo website.\rFill out the metadata One of the first options you need to specify is the “upload type”. For code repositories, you can choose the “software” option. The remaining metadata is relatively simple to fill out (such as author and institution). However, one category to pay attention to is the license: by default the CC-BY-4.0 license is selected. For a short overview of what this means, see the creative commons website: https://creativecommons.org/licenses/by/4.0/. You can opt for a different license by including a file called LICENSE in your repository.\nFigure 5: Selecting the \u0026lsquo;software\u0026rsquo; option for upload type.\rPublish! The last step is to click “publish”. Your research code is now findable, citable, understandable, reproducible, and archived until the end of times! You can now show it to all your colleagues and easily cite it in your manuscript. If you get feedback and you want to change your code, you can also upload a new version of the same project on the Zenodo website.\nConclusion In this post, I described a checklist for preparing your code folder for publication with a focus on understandability, and I have described one way in which you can upload your prepared code repository to an open access archive. Zenodo is an easy, dependable and well-built option, but of course there are many alternatives, such as hosting it on your own website, using the Open Science Framework, GitHub, or using a publisher’s website; each has its own advantages and disadvantages.\n","date":"September 5, 2022","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/tutorials/post-1/","title":"How to share your research code"},{"categories":null,"contents":"During your time as a fellow:\nyou will spend between 3-8 months full-time* working on a social science research project. you can propose your own project, based on your interests. you are a member of the SoDa team at the Methodology \u0026amp; Statistics department of Utrecht University. you will get a salary during this time, paid for by the team. one of the senior team members will be your mentor. To apply, you have to submit a short proposal for your project, together with a substantive supervisor. We are looking for projects in the social sciences for which a computational or data-related problem needs to be solved.\nThe next submission deadline is 31 December 2023\n* part-time possible, but the project should be your main priority.\n","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/fellowship/","title":"Fellowship"},{"categories":null,"contents":"","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/principles/","title":"Principles"},{"categories":null,"contents":"","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/projects/","title":"Projects"},{"categories":null,"contents":"","date":"January 1, 1","image":"http://odissei-soda.nl/images/tutorial-1/tutorial1_header_hu650f89d19acf6379f59d4eaf3a39c00b_29682_650x0_resize_box_3.png","permalink":"/team/","title":"The SoDa Team"}]